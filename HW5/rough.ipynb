{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rudraksh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Include all the libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from statistics import mean\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import string \n",
    "\n",
    "#Sklearn Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Plotting Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0\n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_txt = pd.read_csv(\"train.csv\").fillna('unknown')\n",
    "train_txt = train_txt.drop(['severe_toxic','obscene','threat','insult','identity_hate'],axis=1)\n",
    "train_txt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data with toxic and non-toxic\n",
    "Non_toxic_subset = train_txt[train_txt['toxic']==0]\n",
    "Toxic_subset = train_txt[train_txt['toxic']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean function and for removing stop words\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = ''.join(word for word in text if not word.isdigit())\n",
    "    \"\"\"# Remove double quotes\n",
    "    text = text.replace('\"', ' ')\n",
    "    text = text.replace('“', ' ')\n",
    "    text = text.replace('”', ' ')\n",
    "    # Remove single quotes\n",
    "    text = text.replace(\"'\", \" \")\n",
    "    text = text.replace(\"’\",\" \")\"\"\"\n",
    "\n",
    "    \"\"\"#Removing single character\n",
    "    text = [word for word in text if len(word) > 1]\n",
    "    text = ' '.join(text)\"\"\"\n",
    "\n",
    "    # Remove special characters\n",
    "    text = text.replace('\\r', '').replace('\\n', ' ').replace('\\t', '').strip()\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if not word.lower() in stopwords.words('english')]\n",
    "    # Join the tokens back into a string\n",
    "    clean_text = ' '.join(tokens)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x9/4yztx4d12dj9s__z0mpj3t080000gn/T/ipykernel_35348/3430127188.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Non_toxic_subset['comment_text']= Non_toxic_subset['comment_text'].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "#cleaning toxic and Non_toxic dataset\n",
    "Non_toxic_subset['comment_text']= Non_toxic_subset['comment_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x9/4yztx4d12dj9s__z0mpj3t080000gn/T/ipykernel_35348/23336376.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Toxic_subset['comment_text']= Toxic_subset['comment_text'].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "Toxic_subset['comment_text']= Toxic_subset['comment_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text  toxic\n",
      "0  0000997932d777bf  Explanation edits made username Hardcore Metal...      0\n",
      "1  000103f0d9cfb60f  Daww matches background colour Im seemingly st...      0\n",
      "2  000113f07ec002fd  Hey man Im really trying edit war guy constant...      0\n",
      "3  0001b41b1c6bb37e  cant make real suggestions improvement wondere...      0\n",
      "4  0001d958c54c6e35                sir hero chance remember page thats      0\n"
     ]
    }
   ],
   "source": [
    "print(Non_toxic_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  id                                       comment_text  toxic\n",
      "6   0002bcb3da6cb337                        COCKSUCKER PISS AROUND WORK      1\n",
      "12  0005c987bdfc9d4b  Hey talk exclusive group WP TALIBANSwho good d...      1\n",
      "16  0007e25b2121310b       Bye Dont look come think comming back Tosser      1\n",
      "42  001810bf8c45bf5f  gay antisemmitian Archangel WHite Tiger Meow G...      1\n",
      "43  00190820581d90ce                         FUCK FILTHY MOTHER ASS DRY      1\n"
     ]
    }
   ],
   "source": [
    "print(Toxic_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explanation\n",
      "why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27 d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc) hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talkin\n",
      "cocksucker piss around work hey talk exclusive group wp talibanswho good destroying selfappointed purist gang one asks questions abt antisocial destructive noncontribution wp ask sityush clean behavior issue nonsensical warnings bye dont look come think comming back tosser gay antisemmitian archangel white tiger meow greetingshhh uh two ways erased comment ww holocaust brutally slaying jews gaysgypsysslavsanyone antisemitian shave head bald go skinhead meetings doubt words bible homosexuality de\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the values in the 'text' column into a single string\n",
    "NOT_TOXIC_subset= ' '.join(Non_toxic_subset['comment_text'].astype(str).tolist()).lower()\n",
    "\n",
    "TOXIC_subset= ' '.join(Toxic_subset['comment_text'].astype(str).tolist()).lower()\n",
    "\n",
    "# Print the first 500 characters of the resulting string\n",
    "print(NOT_TOXIC_subset[:500])\n",
    "print(TOXIC_subset[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counter function to get the most K word\n",
    "#it will return the list of most common word \n",
    "from collections import Counter\n",
    "def find_common_word(string_data,k):\n",
    "    split = string_data.split()\n",
    "    Count = Counter(split)\n",
    "    most_occur = Count.most_common(k)\n",
    "    print(most_occur)\n",
    "    most_occur_word = [x[0] for x in most_occur]\n",
    "    return most_occur_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 469724), ('to', 278545), ('of', 211521), ('and', 202637), ('a', 191495), ('i', 177013), ('is', 158663), ('you', 156647), ('that', 139262), ('in', 133994)]\n",
      "[('fuck', 8615), ('shit', 3587), ('dont', 3528), ('like', 3477), ('nigger', 3289), ('wikipedia', 3264), ('fucking', 3194), ('suck', 3036), ('go', 2834), ('hate', 2614)]\n"
     ]
    }
   ],
   "source": [
    "#most common 5 word \n",
    "list1 = find_common_word(NOT_TOXIC_subset,10)\n",
    "list2 = find_common_word(TOXIC_subset,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'to', 'of', 'and', 'a', 'i', 'is', 'you', 'that', 'in']\n"
     ]
    }
   ],
   "source": [
    "print(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caluculate word to vector \n",
    "import gensim.downloader as api\n",
    "#print(list(gensim.downloader.info()['models'].keys()))\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "#print(model.most_similar(\"rudraksh\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def similarity_table(list1, list2, model):\n",
    "    # Generate word embeddings for words in each list\n",
    "    embeddings1 = np.array([model[word] for word in list1 if word in model])\n",
    "    print(embeddings1)\n",
    "    embeddings2 = np.array([model[word] for word in list2 if word in model])\n",
    "\n",
    "    # Calculate cosine similarity between each pair of embeddings\n",
    "    sim_matrix = cosine_similarity(embeddings1, embeddings2)\n",
    "\n",
    "    # Create a pandas DataFrame to display the results\n",
    "    df = pd.DataFrame(sim_matrix, index=list1, columns=list2)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fuck', 'shit', 'dont', 'like', 'nigger', 'wikipedia', 'fucking', 'suck', 'go', 'hate', 'ass', 'u', 'get', 'gay', 'know', 'page', 'die', 'im', 'fat', 'faggot']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (15, 19), indices imply (20, 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x9/4yztx4d12dj9s__z0mpj3t080000gn/T/ipykernel_35348/3283887683.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#print(df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 )\n\u001b[1;32m    671\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    673\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0m_check_values_indices_shape_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mpassed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mimplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (15, 19), indices imply (20, 20)"
     ]
    }
   ],
   "source": [
    "#Caluclating cosine similarity\n",
    "#create a cosine similarity function like above\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(list2)\n",
    "embeddings1 = np.array([model[word] for word in list1 if word in model])\n",
    "embeddings2 = np.array([model[word] for word in list2 if word in model])\n",
    "\n",
    "similarity = cosine_similarity(embeddings1,embeddings2)\n",
    "df = pd.DataFrame(similarity, index=list1, columns=list2)\n",
    "#print(df)\n",
    "\n",
    "average_similarity = np.mean(similarity)\n",
    "print(average_similarity)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task2 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document summary for war of worlds.txt\n",
    "with open(\"warofworlds.txt\", 'r') as f:\n",
    "    contents = f.read().lower()\n",
    "    contents = clean_text(contents)\n",
    "    common_content_word = find_common_word(contents,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'may', 'opinion', 'would', 'others']\n"
     ]
    }
   ],
   "source": [
    "#document summary for war of worlds.txt\n",
    "with open(\"on_liberty.txt\", 'r') as f:\n",
    "    contents = f.read().lower()\n",
    "    contents = clean_text(contents)\n",
    "    common_content_word = find_common_word(contents,5)\n",
    "    print(common_content_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sir', 'king', 'said', 'knight', 'thou', 'knights', 'came', 'ye', 'thee', 'lancelot']\n"
     ]
    }
   ],
   "source": [
    "#document summary for war of worlds.txt\n",
    "with open(\"kingarthur.txt\", 'r') as f:\n",
    "    contents = f.read().lower()\n",
    "    contents = clean_text(contents)\n",
    "    common_content_word = find_common_word(contents,10)\n",
    "    print(common_content_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'upon', 'said', 'martians', 'people']\n",
      "[['only', 'two', 'three', 'five', 'four'], ['on', 'Upon', 'Performance_Ratios_Based', 'onthe', 'whereupon'], ['says', 'explained', 'siad', 'noted', 'stressed'], ['Martians', 'Venusians', 'humanoid_aliens', 'Gungans', 'outerspace'], ['peole', 'poeple', 'individuals', 'folks', 'peple']]\n"
     ]
    }
   ],
   "source": [
    "print(common_content_word)\n",
    "Similar_word = []\n",
    "for i in common_content_word:\n",
    "    temp  = model.most_similar(i,topn=5)\n",
    "    similar = [x[0] for x in temp]\n",
    "    Similar_word.append(similar)\n",
    "print(Similar_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an SVM model with a linear kernel\n",
    "model = SVC(kernel='linear')\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
