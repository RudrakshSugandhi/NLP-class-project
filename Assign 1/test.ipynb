{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open('book.txt', 'r') as f:\n",
    "   contents = f.read().lower()\n",
    "   cont_reg = re.sub(r'([a-z]{1,}(?<!y|h|f|s|c))our', r'\\1or', contents, count=0, flags=0)\n",
    "   #print(cont_reg)\n",
    "   \"\"\"\n",
    "   reg experssion  which is greater than size 5 and contain our should be replaced apart from that None\n",
    "   \\b\\w{1,}our{1,}\\b'\n",
    "   \\b\\w*our\\w*\\b\n",
    "   \\b\\w{6,}.*our.*\\b'\n",
    "   (?<!y)our\n",
    "   #.{6,}.\n",
    "   # *our.* --- Check this how to check for maximum character in string if 5 or more\n",
    "   .{1,}.*our.*\n",
    "   .{1,}our.+ -- can be able to doge your and our but stuck with yourself\n",
    "   .{1,}(?<!y)our.+---- This can dodge all the cases but still fourth and hours are the one\n",
    "   /.{1,}(?<!y|h|f)our.+/ - finally the final regex compup with\n",
    "   in the replacement how to make the changes that other forward and backward remains same\n",
    "   How to write the right replacement\n",
    "   \"\"\"\n",
    "   #(Dr. , Mr., Ms., Mrs.) with appropriate expansions of words (Doctor, Mister, Miss, Misses)\n",
    "   rep = {'dr.':'doctor','mr.':'mister','ms.':\"miss\",\"mrs.\":\"misses\"}\n",
    "   for key in rep.keys():\n",
    "      cont_reg= cont_reg.replace(key,rep[key])\n",
    "\n",
    "with open('regex.txt', 'w') as f:\n",
    "   print(cont_reg,file=f)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book is called yourself and our is the only word with neighbor\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "str1 = \"The book is called yourself and our is the only word with neighbour\"\n",
    "cont = re.sub(r'([a-z]{1,}(?<!y|h|f|s|c))our', r'\\1or', str1, count=0, flags=0)\n",
    "print(cont)\n",
    "\n",
    "#([a-z]{1,}(?<!y|h|f|s|c))our-----\n",
    "#(.{1,}(?<!y|h|f))our(.+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "{'other', 'couldn', 'until', 'll', 've', \"weren't\", 'them', 'each', 'below', 'shan', 'both', 'on', 'm', 'themselves', 'ours', 'i', 'against', 'their', 'hadn', 'same', 'only', \"mustn't\", 'my', 'they', 'of', \"wasn't\", 'between', 'an', 'wasn', 'weren', 'myself', 'have', 'where', 'are', 'his', 'himself', 'that', 'to', 'here', 'further', 'she', \"hadn't\", 'her', \"don't\", 'such', 'hers', 'before', 's', 'do', 'mustn', \"shan't\", 'aren', 're', 'we', 'me', 'having', 'very', \"that'll\", 'he', 'itself', 'again', 'its', 'down', \"shouldn't\", 'our', 'any', 'some', 'how', 'was', 'whom', 'than', 'as', \"haven't\", 'not', \"should've\", \"hasn't\", 'by', 'a', 'out', 'few', 'am', 'most', 'can', 'wouldn', 'mightn', 'did', 'now', 'should', 'didn', 'because', 'this', 'if', 'doesn', 'all', \"you've\", 'why', 'but', 'once', 'through', 'then', 'won', 'during', 'theirs', 'nor', 'over', 'does', 'ourselves', 'being', 'after', 'your', 'yourself', 'what', \"mightn't\", 'needn', 'who', 'd', \"she's\", 'is', 'from', 'were', 'when', 'y', \"couldn't\", 'those', 'o', \"doesn't\", 'been', 'the', 'ain', 'no', \"isn't\", 'don', 'about', 'up', 'isn', 'you', 't', \"you're\", 'these', 'shouldn', 'doing', 'will', 'had', 'haven', 'at', 'so', 'hasn', \"it's\", \"wouldn't\", \"needn't\", 'with', 'in', \"you'll\", 'ma', 'above', 'too', \"didn't\", 'has', 'herself', \"won't\", 'it', \"aren't\", 'which', 'him', 'under', 'off', 'or', 'into', 'while', 'yourselves', 'be', 'and', 'more', 'yours', 'there', 'just', 'own', \"you'd\", 'for'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rudraksh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "#removing the NLTK library\n",
    "#think about stemming and lemmatization \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#refer this website for normalization: https://www.geeksforgeeks.org/normalizing-textual-data-with-python/\n",
    "with open('regex.txt', 'r') as f:\n",
    "    # lower case\n",
    "    contents = f.read().lower()\n",
    "    #removing numbers\n",
    "    contents = re.sub(r'\\d+','',contents)\n",
    "    #removing puctuation\n",
    "    contents = re.sub(r'[^\\w\\s]','',contents)\n",
    "    #removing white space \n",
    "    contents = contents.strip(\" \")\n",
    "    #removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    contents = [contents][0].split()\n",
    "    content_w_stpword = []\n",
    "    for i in contents:\n",
    "        if i not in stop_words:\n",
    "            content_w_stpword.append(i)\n",
    "    #remove duplicates\n",
    "    content_w_stpword = sorted(set(content_w_stpword))\n",
    "    upt_content = []\n",
    "    # '_' removing the special character from the strings\n",
    "    for i in content_w_stpword:\n",
    "        replace = i.replace(\"_\",\"\")\n",
    "        upt_content.append(replace)\n",
    "    # re-sort after updates\n",
    "    updated_content = sorted(upt_content)\n",
    "    #print(updated_content)\n",
    "    #my_dict = dict()   \n",
    "    #for index, value in enumerate(updated_content):\n",
    "    #    my_dict[index] = value\n",
    "    #print(my_dict)\n",
    "with open('dictionary.txt', 'w') as f:\n",
    "    for item in updated_content:\n",
    "        f.write(\"%s\\n\"% item)\n",
    "    print(\"Done\")\n",
    "print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7973e275cb3cbbcad93b5a1192ab34ec2c2aa349b94e548542cf66bb6bf59ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
